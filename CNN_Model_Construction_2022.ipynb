{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlzj7rZtQa7N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Dense, Dropout, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Configuration"
      ],
      "metadata": {
        "id": "iqunGyoZU_ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel('C:/users/a.aybarf/Downloads/dfMLplp2022.xlsx', index_col=0)"
      ],
      "metadata": {
        "id": "xNMLW8FlQblP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df1)"
      ],
      "metadata": {
        "id": "zIB-2GimQbpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "j2rj7SnzQbrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns = ['plp',                 # Likely an acronym (e.g., Poverty Line Percentage) – keep as-is unless you know the meaning\\n\",\n",
        "    'urban_rural',         # 'rururb' = rural or urban\\n\",\n",
        "    'age',                 # 'edad'\\n\",\n",
        "    'sex',                 # 'sexo'\\n\",\n",
        "    'literacy',            # 'alfabetism'\\n\",\n",
        "    'trabajo_mp',\n",
        "    'food_insecurity',     # 'ins_ali' = food insecurity\\n\",\n",
        "    'hli',                 # Likely an acronym – keep as-is unless clarified\\n\",\n",
        "    'public_healthcare',   # 'segpop' = public health insurance or healthcare\\n\",\n",
        "    'medical_attention',   # 'atemed'\\n\",\n",
        "    'bank_card',           # 'tarjeta' = likely a debit/credit/bank card\\n\",\n",
        "    'electricity',  # 'disp_elect' = electronic devices availability\\n\",\n",
        "    'total_residents',     # 'tot_resid'\\n\",\n",
        "    'region',              # 'región'\\n\",\n",
        "    'connectivity',        # 'conectividad' = internet or digital connectivity\\n\",\n",
        "    'water_drainage',      # 'agua_drenaje'\\n\",\n",
        "    'household_head_edu',  # 'neducativojefe' = educational level of household head\\n\",\n",
        "    'child_labor',         # 'trabajomenores'\\n\",\n",
        "    'children',            # 'niños'\\n\",\n",
        "    'household_occupation', # 'ocupacion_hogar' = economic activity of the household\\n\",\n",
        "    'consumption expenditure',\n",
        "    'housing_tenure',\n",
        "    'basic_energy_equipment'\n",
        "   ]"
      ],
      "metadata": {
        "id": "zSIqDZoLQbtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "X_4K3LcwRd62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['urban_rural', 'age', 'sex', 'literacy',\n",
        "       'food_insecurity', 'hli', 'public_healthcare', 'medical_attention',\n",
        "       'bank_card', 'electricity', 'total_residents', 'region', 'connectivity',\n",
        "       'water_drainage', 'household_head_edu', 'child_labor', 'children',\n",
        "       'household_occupation', 'consumption expenditure', 'housing_tenure',\n",
        "       'basic_energy_equipment']]\n",
        "y = data[['plp']]"
      ],
      "metadata": {
        "id": "lh0k3ptCRmvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Optimizacion During Construction"
      ],
      "metadata": {
        "id": "HWnRvyEXR5v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train1, X_test1, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45) #hacerle reshape a todo (train, test y validation)\n",
        "\n",
        "X_train = X_train1.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True) #dividir a y_0 y y_1\n",
        "\n",
        "X_test_original = X_test1\n",
        "X_test_originalnumpy = X_test1.to_numpy()\n",
        "\n",
        "Xtrainfinal1 = X_train.to_numpy()\n",
        "ytrainfinal1 = y_train.to_numpy()\n",
        "X_test1 = X_test1.to_numpy()\n",
        "y_test1 = y_test.to_numpy()\n",
        "\n",
        "X_trainP=tf.reshape(Xtrainfinal1,(Xtrainfinal1.shape[0], Xtrainfinal1.shape[1],1))\n",
        "\n",
        "X_test1=tf.reshape(X_test1,(X_test1.shape[0], X_test1.shape[1],1))"
      ],
      "metadata": {
        "id": "3ClKKpW9Rmxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_learning_rate = 0.01\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=200, decay_rate=0.9)"
      ],
      "metadata": {
        "id": "2V8-gqTuRmzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling1D"
      ],
      "metadata": {
        "id": "3LTHbYKLReA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter Values Testing"
      ],
      "metadata": {
        "id": "OOa_a5SOU1X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de configuraciones de filtros para los modelos (Tabla 6)\n",
        "filter_configs = [\n",
        "    {'conv_filters': [75, 50, 25], 'dense_units': [25, 1]},\n",
        "    {'conv_filters': [50, 25, 10], 'dense_units': [10, 1]},\n",
        "    {'conv_filters': [30, 15, 5],  'dense_units': [10, 1]},\n",
        "    {'conv_filters': [20, 10, 5],  'dense_units': [5, 1]},\n",
        "    {'conv_filters': [100, 50, 25],'dense_units': [20, 1]},\n",
        "    {'conv_filters': [120, 100, 120],'dense_units': [40, 1]},\n",
        "    {'conv_filters': [150, 100, 50],'dense_units': [25, 1]},\n",
        "    {'conv_filters': [20, 10, 5],  'dense_units': [10, 1]}\n",
        "]\n",
        "\n",
        "for model_idx, config in enumerate(filter_configs, start=1):\n",
        "    print(f'\\nTesting Model {model_idx} with filters: {config[\"conv_filters\"]} and dense units: {config[\"dense_units\"]}')\n",
        "\n",
        "    def create_cnn_model(X_trainP):\n",
        "        model = Sequential()\n",
        "        # Capas convolucionales y de pooling\n",
        "        model.add(Conv1D(filters=config['conv_filters'][0], kernel_size=3, activation='relu', input_shape=(X_trainP.shape[1], X_trainP.shape[2]), padding='same'))\n",
        "        model.add(Conv1D(filters=config['conv_filters'][1], kernel_size=3, activation='relu', padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Conv1D(filters=config['conv_filters'][2], kernel_size=3, activation='relu', padding='same'))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        # Capas densas\n",
        "        model.add(Dense(config['dense_units'][0], activation='relu'))\n",
        "        model.add(Dense(config['dense_units'][1], activation='sigmoid'))\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    for i in range(5):  # 5 repeticiones\n",
        "        best_model = None\n",
        "        best_accuracy = 0.0\n",
        "        counter = 1\n",
        "        accuracyacumulada = 0.0\n",
        "\n",
        "        for train_idx, val_idx in kfold.split(Xtrainfinal1, ytrainfinal1):\n",
        "            dfX = pd.DataFrame(Xtrainfinal1, columns=X.columns)\n",
        "            dfy = pd.DataFrame(ytrainfinal1, columns=y.columns)\n",
        "\n",
        "            X_trainP, X_val = dfX.loc[train_idx], dfX.loc[val_idx]\n",
        "            y_trainP, y_val = dfy.loc[train_idx], dfy.loc[val_idx]\n",
        "\n",
        "            X_trainP = X_trainP.to_numpy()\n",
        "            X_val = X_val.to_numpy()\n",
        "            y_trainP = y_trainP.to_numpy()\n",
        "            y_val = y_val.to_numpy()\n",
        "\n",
        "            X_trainP = tf.reshape(X_trainP, (X_trainP.shape[0], X_trainP.shape[1], 1))\n",
        "            X_val = tf.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
        "\n",
        "            model = create_cnn_model(X_trainP)\n",
        "            history = model.fit(X_trainP, y_trainP, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "            y_val_pred = model.predict(X_val)\n",
        "            y_val_pred = (y_val_pred > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "            counter += 1\n",
        "            accuracyacumulada += accuracy\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model = model\n",
        "                best_history = history\n",
        "\n",
        "        print(f'{i+1}: Model {model_idx} - Best Validation Accuracy: {best_accuracy * 100:.2f}%')\n",
        "\n",
        "        y_test_pred = best_model.predict(X_test1)\n",
        "        y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "        test_accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "    print('####################################################')"
      ],
      "metadata": {
        "id": "TcWZgc7MSRgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kernel Size"
      ],
      "metadata": {
        "id": "g3UeShvOSafz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernellist = [2,3,5,7,11]\n",
        "\n",
        "for kernelsize in kernellist:\n",
        "    print(f'Test for Kernel Size: {kernelsize}')\n",
        "    # Define the CNN model\n",
        "    def create_cnn_model(X_trainP):\n",
        "        model = Sequential()\n",
        "        # Capas convolucionales y de pooling\n",
        "        model.add(Conv1D(filters=120, kernel_size=kernelsize, activation='relu', input_shape=(X_trainP.shape[1], X_trainP.shape[2]), padding='same'))\n",
        "        model.add(Conv1D(filters=100, kernel_size=kernelsize, activation='relu', padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Conv1D(filters=120, kernel_size=kernelsize, activation='relu', padding='same'))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        #Capas densas\n",
        "        model.add(Dense(10, activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy']) #sparse_categorical_crossentropy (si es que no funciona la división de y con one-hot encoding)\n",
        "        return model\n",
        "\n",
        "    for i in range(0,5):\n",
        "        best_model = None\n",
        "        best_accuracy = 0.0\n",
        "        counter = 1\n",
        "        accuracyacumulada = 0.0\n",
        "        # Perform 10-fold cross-validation\n",
        "        for train_idx, val_idx in kfold.split(Xtrainfinal1, ytrainfinal1):\n",
        "\n",
        "            dfX = pd.DataFrame(Xtrainfinal1, columns=X.columns)\n",
        "            dfy = pd.DataFrame(ytrainfinal1, columns=y.columns)\n",
        "\n",
        "            X_trainP, X_val = dfX.loc[train_idx], dfX.loc[val_idx]\n",
        "            y_trainP, y_val = dfy.loc[train_idx], dfy.loc[val_idx]\n",
        "\n",
        "            X_trainP = X_trainP.to_numpy()\n",
        "            X_val = X_val.to_numpy()\n",
        "            y_trainP = y_trainP.to_numpy()\n",
        "            y_val = y_val.to_numpy()\n",
        "\n",
        "            X_trainP=tf.reshape(X_trainP,(X_trainP.shape[0], X_trainP.shape[1],1))\n",
        "            X_val=tf.reshape(X_val,(X_val.shape[0], X_val.shape[1],1))\n",
        "\n",
        "            model = create_cnn_model(X_trainP)\n",
        "            history = model.fit(X_trainP, y_trainP, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=0) #100 epochs\n",
        "            #history\n",
        "            #(sacarlo primero del for para probar la arquitecutra y luego incluirlo)\n",
        "\n",
        "            # Evaluate the model on the validation set\n",
        "            y_val_pred = model.predict(X_val)\n",
        "            y_val_pred = (y_val_pred > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_val, y_val_pred)\n",
        "            #print(f'Validation Accuracy {counter}: {accuracy * 100:.2f}%')\n",
        "\n",
        "            counter += 1\n",
        "            accuracyacumulada += accuracy\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model = model\n",
        "                best_history = history\n",
        "\n",
        "        #print(' ')\n",
        "        #print(f'Average Validation Accuracy: {(accuracyacumulada/10) * 100:.2f}%')\n",
        "        print(f'{i+1}: {kernelsize}')\n",
        "\n",
        "        print(f'Best Validation Accuracy: {best_accuracy * 100:.2f}%')\n",
        "\n",
        "        # Make predictions on the test set using the best model\n",
        "        y_test_pred = best_model.predict(X_test1)\n",
        "\n",
        "        y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "        # Evaluate the performance on the test set\n",
        "        test_accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "    print('####################################################')"
      ],
      "metadata": {
        "id": "tU7I8XSDR33X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN Arquitecture"
      ],
      "metadata": {
        "id": "NjsqVgADVpJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architectures = {\n",
        "    'A1': lambda input_shape: build_model_A1(input_shape),\n",
        "    'A2': lambda input_shape: build_model_A2(input_shape),\n",
        "    'A3': lambda input_shape: build_model_A3(input_shape),\n",
        "    'A4': lambda input_shape: build_model_A4(input_shape),\n",
        "    'A5': lambda input_shape: build_model_A5(input_shape, 0.1),\n",
        "    'A6': lambda input_shape: build_model_A5(input_shape, 0.2),\n",
        "    'A7': lambda input_shape: build_model_A7(input_shape, 0.1),\n",
        "    'A8': lambda input_shape: build_model_A8_A10(input_shape, 0.1, use_batchnorm=True, use_flatten=True, use_avgpool=False),\n",
        "    'A9': lambda input_shape: build_model_A8_A10(input_shape, 0.1, use_batchnorm=True, use_flatten=False, use_avgpool=True),\n",
        "    'A10': lambda input_shape: build_model_A8_A10(input_shape, 0.1, use_batchnorm=True, use_flatten=True, use_avgpool=True)\n",
        "}\n",
        "\n",
        "# Funciones para construir cada arquitectura\n",
        "def build_model_A1(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(120, 3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(40, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_A2(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(120, 3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(40, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_A3(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(120, 3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(60, activation='relu'),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dense(40, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_A4(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(120, 3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(60, activation='relu'),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dense(40, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_A5(input_shape, dropout_rate):\n",
        "    model = Sequential([\n",
        "        Conv1D(120, 3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        Conv1D(100, 3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(120, 3, activation='relu', padding='same'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(40, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_A7(input_shape, dropout_rate):\n",
        "    model = Sequential([\n",
        "        Conv1D(120, 3, padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(), Activation('relu'),\n",
        "        Conv1D(100, 3, padding='same'),\n",
        "        BatchNormalization(), Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(120, 3, padding='same'),\n",
        "        BatchNormalization(), Activation('relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(40), BatchNormalization(), Activation('relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_A8_A10(input_shape, dropout_rate, use_batchnorm=True, use_flatten=False, use_avgpool=False):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(120, 3, padding='same', input_shape=input_shape))\n",
        "    if use_batchnorm: model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv1D(100, 3, padding='same'))\n",
        "    if use_batchnorm: model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    if use_avgpool:\n",
        "        model.add(AveragePooling1D(pool_size=2))\n",
        "    else:\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Conv1D(120, 3, padding='same'))\n",
        "    if use_batchnorm: model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    if use_flatten:\n",
        "        model.add(Flatten())\n",
        "    else:\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    model.add(Dense(40))\n",
        "    if use_batchnorm: model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "iAM1gMApR35Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for arch_name, build_fn in architectures.items():\n",
        "    print(f'\\nTesting Architecture {arch_name}')\n",
        "\n",
        "    for i in range(5):  # 5 repeticiones\n",
        "        best_model = None\n",
        "        best_accuracy = 0.0\n",
        "        accuracyacumulada = 0.0\n",
        "\n",
        "        for train_idx, val_idx in kfold.split(Xtrainfinal1, ytrainfinal1):\n",
        "            dfX = pd.DataFrame(Xtrainfinal1, columns=X.columns)\n",
        "            dfy = pd.DataFrame(ytrainfinal1, columns=y.columns)\n",
        "\n",
        "            X_trainP, X_val = dfX.loc[train_idx], dfX.loc[val_idx]\n",
        "            y_trainP, y_val = dfy.loc[train_idx], dfy.loc[val_idx]\n",
        "\n",
        "            X_trainP = tf.reshape(X_trainP.to_numpy(), (len(X_trainP), X_trainP.shape[1], 1))\n",
        "            X_val = tf.reshape(X_val.to_numpy(), (len(X_val), X_val.shape[1], 1))\n",
        "            y_trainP = y_trainP.to_numpy()\n",
        "            y_val = y_val.to_numpy()\n",
        "\n",
        "            model = build_fn((X_trainP.shape[1], X_trainP.shape[2]))\n",
        "            history = model.fit(X_trainP, y_trainP, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "            y_val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_val, y_val_pred)\n",
        "            accuracyacumulada += accuracy\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model = model\n",
        "\n",
        "        print(f'{i+1}: {arch_name} - Best Validation Accuracy: {best_accuracy * 100:.2f}%')\n",
        "        y_test_pred = (best_model.predict(X_test1) > 0.5).astype(int)\n",
        "        test_accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "    print('####################################################')"
      ],
      "metadata": {
        "id": "qUYRdzqMTBGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Optimizacion During Training"
      ],
      "metadata": {
        "id": "5H9tDO5PVRux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "def create_cnn_model(Xtrainfinal1):\n",
        "    model = Sequential()\n",
        "    # Capas convolucionales y de pooling\n",
        "    model.add(Conv1D(filters=120, kernel_size=3, activation='relu', input_shape=(Xtrainfinal1.shape[1], Xtrainfinal1.shape[2]), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=100, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=120, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    #Capas densas\n",
        "    model.add(Dense(40, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))  # Add dropout for regularization\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy']) #sparse_categorical_crossentropy (si es que no funciona la división de y con one-hot encoding)\n",
        "    return model"
      ],
      "metadata": {
        "id": "j_qKVCKFTBJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epoch Values Testing"
      ],
      "metadata": {
        "id": "oVzgmQqbZmAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epovalue in [25,50,100,150,200]:\n",
        "    print(f'Test for Epoch: {epovalue}')\n",
        "    for i in range(0,5):\n",
        "        best_model = None\n",
        "        best_accuracy = 0.0\n",
        "        counter = 1\n",
        "        accuracyacumulada = 0.0\n",
        "        # Perform 10-fold cross-validation\n",
        "        for train_idx, val_idx in kfold.split(Xtrainfinal1, ytrainfinal1):\n",
        "\n",
        "            dfX = pd.DataFrame(Xtrainfinal1, columns=X.columns)\n",
        "            dfy = pd.DataFrame(ytrainfinal1, columns=y.columns)\n",
        "\n",
        "            X_trainP, X_val = dfX.loc[train_idx], dfX.loc[val_idx]\n",
        "            y_trainP, y_val = dfy.loc[train_idx], dfy.loc[val_idx]\n",
        "\n",
        "            X_trainP = X_trainP.to_numpy()\n",
        "            X_val = X_val.to_numpy()\n",
        "            y_trainP = y_trainP.to_numpy()\n",
        "            y_val = y_val.to_numpy()\n",
        "\n",
        "            X_trainP=tf.reshape(X_trainP,(X_trainP.shape[0], X_trainP.shape[1],1))\n",
        "            X_val=tf.reshape(X_val,(X_val.shape[0], X_val.shape[1],1))\n",
        "\n",
        "            model = create_cnn_model(X_trainP)\n",
        "            history = model.fit(X_trainP, y_trainP, epochs=epovalue, batch_size=32, validation_data=(X_val, y_val), verbose=0) #100 epochs\n",
        "            history\n",
        "            #(sacarlo primero del for para probar la arquitecutra y luego incluirlo)\n",
        "\n",
        "            # Evaluate the model on the validation set\n",
        "            y_val_pred = model.predict(X_val)\n",
        "            y_val_pred = (y_val_pred > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_val, y_val_pred)\n",
        "            #print(f'Validation Accuracy {counter}: {accuracy * 100:.2f}%')\n",
        "\n",
        "            counter += 1\n",
        "            accuracyacumulada += accuracy\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model = model\n",
        "                best_history = history\n",
        "\n",
        "        #print(' ')\n",
        "        #print(f'Average Validation Accuracy: {(accuracyacumulada/10) * 100:.2f}%')\n",
        "        print(f'{i+1}: {epovalue}')\n",
        "\n",
        "        print(f'Best Validation Accuracy: {best_accuracy * 100:.2f}%')\n",
        "\n",
        "        # Make predictions on the test set using the best model\n",
        "        y_test_pred = best_model.predict(X_test1)\n",
        "\n",
        "        y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "        # Evaluate the performance on the test set\n",
        "        test_accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "    print('####################################################')"
      ],
      "metadata": {
        "id": "xbsinwdWTBLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batch Sizes Values Testing"
      ],
      "metadata": {
        "id": "2P61Q7rSZykB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batchvalue in [32,64,96,128,160]:\n",
        "    print(f'Test for Batch Size: {batchvalue}')\n",
        "    for i in range(0,5):\n",
        "        best_model = None\n",
        "        best_accuracy = 0.0\n",
        "        counter = 1\n",
        "        accuracyacumulada = 0.0\n",
        "        # Perform 10-fold cross-validation\n",
        "        for train_idx, val_idx in kfold.split(Xtrainfinal1, ytrainfinal1):\n",
        "\n",
        "            dfX = pd.DataFrame(Xtrainfinal1, columns=X.columns)\n",
        "            dfy = pd.DataFrame(ytrainfinal1, columns=y.columns)\n",
        "\n",
        "            X_trainP, X_val = dfX.loc[train_idx], dfX.loc[val_idx]\n",
        "            y_trainP, y_val = dfy.loc[train_idx], dfy.loc[val_idx]\n",
        "\n",
        "            X_trainP = X_trainP.to_numpy()\n",
        "            X_val = X_val.to_numpy()\n",
        "            y_trainP = y_trainP.to_numpy()\n",
        "            y_val = y_val.to_numpy()\n",
        "\n",
        "            X_trainP=tf.reshape(X_trainP,(X_trainP.shape[0], X_trainP.shape[1],1))\n",
        "            X_val=tf.reshape(X_val,(X_val.shape[0], X_val.shape[1],1))\n",
        "\n",
        "            model = create_cnn_model(X_trainP)\n",
        "            history = model.fit(X_trainP, y_trainP, epochs=100, batch_size=batchvalue, validation_data=(X_val, y_val), verbose=0) #100 epochs\n",
        "            #history\n",
        "            #(sacarlo primero del for para probar la arquitecutra y luego incluirlo)\n",
        "\n",
        "            # Evaluate the model on the validation set\n",
        "            y_val_pred = model.predict(X_val)\n",
        "            y_val_pred = (y_val_pred > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_val, y_val_pred)\n",
        "            #print(f'Validation Accuracy {counter}: {accuracy * 100:.2f}%')\n",
        "\n",
        "            counter += 1\n",
        "            accuracyacumulada += accuracy\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model = model\n",
        "                best_history = history\n",
        "\n",
        "        #print(' ')\n",
        "        #print(f'Average Validation Accuracy: {(accuracyacumulada/10) * 100:.2f}%')\n",
        "        print(f'{i+1}: {batchvalue}')\n",
        "\n",
        "        print(f'Best Validation Accuracy: {best_accuracy * 100:.2f}%')\n",
        "\n",
        "        # Make predictions on the test set using the best model\n",
        "        y_test_pred = best_model.predict(X_test1)\n",
        "\n",
        "        y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "        # Evaluate the performance on the test set\n",
        "        test_accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "    print('####################################################')"
      ],
      "metadata": {
        "id": "b13ITirlTBPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final CNN Model Evaluation"
      ],
      "metadata": {
        "id": "dzDhNa1VaAkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train1, X_test1, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45) #hacerle reshape a todo (train, test y validation)\n",
        "\n",
        "X_train = X_train1.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True) #dividir a y_0 y y_1\n",
        "\n",
        "Xtrainfinal1 = X_train.to_numpy()\n",
        "ytrainfinal1 = y_train.to_numpy()\n",
        "X_test1 = X_test1.to_numpy()\n",
        "y_test1 = y_test.to_numpy()\n",
        "\n",
        "X_test1=tf.reshape(X_test1,(X_test1.shape[0], X_test1.shape[1],1))"
      ],
      "metadata": {
        "id": "HVfBXIeyTBRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize StratifiedKFold for 10-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=45)"
      ],
      "metadata": {
        "id": "suinnYzhTBTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_learning_rate = 0.01\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=200, decay_rate=0.9)"
      ],
      "metadata": {
        "id": "G_Eiw52caI0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score"
      ],
      "metadata": {
        "id": "J9vJbbXLReLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "def create_cnn_model(Xtrainfinal1):\n",
        "    model = Sequential()\n",
        "    # Capas convolucionales y de pooling\n",
        "    model.add(Conv1D(filters=120, kernel_size=3, activation='relu', input_shape=(Xtrainfinal1.shape[1], Xtrainfinal1.shape[2]), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=100, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=120, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    #Capas densas\n",
        "    model.add(Dense(40, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))  # Add dropout for regularization\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy']) #sparse_categorical_crossentropy (si es que no funciona la división de y con one-hot encoding)\n",
        "    return model\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "counter = 1\n",
        "accuracyacumulada = 0.0\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "for train_idx, val_idx in kfold.split(Xtrainfinal1, ytrainfinal1):\n",
        "\n",
        "    dfX = pd.DataFrame(Xtrainfinal1, columns=X.columns)\n",
        "    dfy = pd.DataFrame(ytrainfinal1, columns=y.columns)\n",
        "\n",
        "    X_trainP, X_val = dfX.loc[train_idx], dfX.loc[val_idx]\n",
        "    y_trainP, y_val = dfy.loc[train_idx], dfy.loc[val_idx]\n",
        "\n",
        "    X_trainP = X_trainP.to_numpy()\n",
        "    X_val = X_val.to_numpy()\n",
        "    y_trainP = y_trainP.to_numpy()\n",
        "    y_val = y_val.to_numpy()\n",
        "\n",
        "    X_trainP=tf.reshape(X_trainP,(X_trainP.shape[0], X_trainP.shape[1],1))\n",
        "    X_val=tf.reshape(X_val,(X_val.shape[0], X_val.shape[1],1))\n",
        "\n",
        "    model = create_cnn_model(X_trainP)\n",
        "    history = model.fit(X_trainP, y_trainP, epochs=50, batch_size=96, validation_data=(X_val, y_val), verbose=0) #100 epochs\n",
        "    #history\n",
        "    #(sacarlo primero del for para probar la arquitecutra y luego incluirlo)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_val_pred = (y_val_pred > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    print(f'Validation Accuracy {counter}: {accuracy * 100:.2f}%')\n",
        "\n",
        "    counter += 1\n",
        "    accuracyacumulada += accuracy\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "        best_history = history\n",
        "        bestcm = confusion_matrix(y_val, y_val_pred)\n",
        "        best_precision = precision_score(y_val, y_val_pred)\n",
        "        bestrecall = recall_score(y_val, y_val_pred)\n",
        "        bestspecificity = bestcm[0, 0] / (bestcm[0, 0] + bestcm[0, 1])\n",
        "        bestf1 = f1_score(y_val, y_val_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_val_pred)\n",
        "        bestroc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "print(' ')\n",
        "print(f'Average Validation Accuracy: {(accuracyacumulada/10) * 100:.2f}%')"
      ],
      "metadata": {
        "id": "68cPlMJ7aKVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best Validation Accuracy: {best_accuracy * 100:.2f}%')\n",
        "print(f'Best Validation Precision: {best_precision * 100:.2f}%')\n",
        "print(f'Best Validation Recall: {bestrecall * 100:.2f}%')\n",
        "print(f'Best Validation Specificity: {bestspecificity * 100:.2f}%')\n",
        "print(f'Best Validation F1: {bestf1:.2f}%')\n",
        "print(f'Best Validation AUC: {bestroc_auc:.2f}%')"
      ],
      "metadata": {
        "id": "a6FrAV56Qbvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the current learning rate from the optimizer\n",
        "current_learning_rate = tf.keras.backend.get_value(model.optimizer.learning_rate)\n",
        "print(\"Current Learning Rate:\", current_learning_rate)"
      ],
      "metadata": {
        "id": "-TKmC2Kmac5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydot\n",
        "import graphviz\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "4JRw9VA-aeL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting both training accuracy and loss in a single graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training accuracy and loss in the same graph\n",
        "plt.plot(best_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(best_history.history['loss'], label='Train Loss')\n",
        "\n",
        "plt.title('Model Accuracy and Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.legend(loc='upper left')"
      ],
      "metadata": {
        "id": "RDWupwYiafQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set using the best model\n",
        "y_test_pred = best_model.predict(X_test1)\n",
        "\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "test_accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "5qXgd3VDafSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "confusion = confusion_matrix(y_test1, y_test_pred)\n",
        "confusion"
      ],
      "metadata": {
        "id": "6-FUulwYahdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Visualize confusion matrix using seaborn\n",
        "sns.set(font_scale=1.2)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            annot_kws={\"size\": 15}, xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')"
      ],
      "metadata": {
        "id": "41vQQ9zsahgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test1, y_test_pred)\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test1, y_test_pred)\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test1, y_test_pred)\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(y_test1, y_test_pred)\n",
        "# Calculate specificity\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test1, y_test_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "-mboS53wajUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calcula el AUC y grafica la curva ROC\n",
        "def plot_roc_curve(y_true, y_scores, title=\"ROC Curve\"):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    return roc_auc\n",
        "\n",
        "# Evaluar AUC en el conjunto de validación y graficar para la última iteración del cross-validation\n",
        "y_val_scores = best_model.predict(X_val).ravel()  # Probabilidades para el conjunto de validación\n",
        "auc_val = plot_roc_curve(y_val, y_val_scores, title=\"ROC Curve (Validation Set)\")\n",
        "\n",
        "# Evaluar AUC en el conjunto de prueba\n",
        "y_test_scores = best_model.predict(X_test1).ravel()  # Probabilidades para el conjunto de prueba\n",
        "auc_test = plot_roc_curve(y_test1, y_test_scores, title=\"ROC Curve (Test Set)\")\n",
        "\n",
        "print(f'Validation AUC: {auc_val:.2f}')\n",
        "print(f'Test AUC: {auc_test:.2f}')"
      ],
      "metadata": {
        "id": "Ndk2CFR3ajWP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}